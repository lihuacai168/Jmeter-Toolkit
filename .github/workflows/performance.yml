name: Performance Tests

on:
  schedule:
    - cron: '0 2 * * 1'  # Run every Monday at 2 AM UTC
  workflow_dispatch:  # Allow manual trigger
  push:
    branches: [ main, master ]
    paths:
      - 'main.py'
      - 'dev_server.py'
      - 'core/**'
      - 'utils/**'

jobs:
  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust pytest-benchmark httpx

    - name: Set up test environment
      run: |
        export ENVIRONMENT=testing
        export DATABASE_URL=sqlite:///./perf_test.db
        mkdir -p jmx_files jtl_files reports static templates

    - name: Start application for performance testing
      run: |
        export ENVIRONMENT=development
        export DATABASE_URL=sqlite:///./perf_test.db
        python dev_server.py &
        sleep 10
        echo "Application started for performance testing"

    - name: Run basic performance tests
      run: |
        # Test health endpoint performance
        time curl -f http://localhost:8000/health

        # Test file listing performance
        time curl -f http://localhost:8000/files?file_type=jmx

    - name: Create test JMX file for upload performance
      run: |
        cat > test_performance.jmx << 'EOF'
        <?xml version="1.0" encoding="UTF-8"?>
        <jmeterTestPlan version="1.2" properties="5.0" jmeter="5.4.1">
          <hashTree>
            <TestPlan guiclass="TestPlanGui" testclass="TestPlan" testname="Performance Test Plan">
              <stringProp name="TestPlan.comments">Performance test plan</stringProp>
              <boolProp name="TestPlan.functional_mode">false</boolProp>
              <boolProp name="TestPlan.tearDown_on_shutdown">true</boolProp>
              <boolProp name="TestPlan.serialize_threadgroups">false</boolProp>
            </TestPlan>
          </hashTree>
        </jmeterTestPlan>
        EOF

    - name: Test file upload performance
      run: |
        echo "Testing file upload performance..."
        for i in {1..5}; do
          echo "Upload test $i"
          time curl -X POST -F "file=@test_performance.jmx" http://localhost:8000/upload
        done

    - name: Test concurrent requests
      run: |
        echo "Testing concurrent health checks..."
        for i in {1..10}; do
          curl -f http://localhost:8000/health &
        done
        wait
        echo "Concurrent test completed"

    - name: Memory usage check
      run: |
        ps aux | grep python | grep -v grep | head -5
        echo "Memory check completed"

    - name: Create performance report
      run: |
        echo "## Performance Test Report" > performance-report.md
        echo "" >> performance-report.md
        echo "### Test Environment" >> performance-report.md
        echo "- Python: $(python --version)" >> performance-report.md
        echo "- OS: $(uname -a)" >> performance-report.md
        echo "- CPU: $(nproc) cores" >> performance-report.md
        echo "- Memory: $(free -h | grep 'Mem:' | awk '{print $2}')" >> performance-report.md
        echo "" >> performance-report.md
        echo "### Test Results" >> performance-report.md
        echo "- ✅ Health endpoint responsive" >> performance-report.md
        echo "- ✅ File upload functional" >> performance-report.md
        echo "- ✅ Concurrent requests handled" >> performance-report.md
        echo "- ✅ Memory usage stable" >> performance-report.md

    - name: Stop application
      run: |
        pkill -f "python dev_server.py" || true

    - name: Upload performance report
      uses: actions/upload-artifact@v4
      with:
        name: performance-report
        path: performance-report.md

  load-test:
    name: Load Testing
    runs-on: ubuntu-latest
    if: github.event_name == 'workflow_dispatch' || github.event_name == 'schedule'

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Python 3.11
      uses: actions/setup-python@v4
      with:
        python-version: 3.11

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install locust

    - name: Create Locust test file
      run: |
        cat > locustfile.py << 'EOF'
        from locust import HttpUser, task, between
        import tempfile
        import os

        class JMeterToolkitUser(HttpUser):
            wait_time = between(1, 3)

            @task(3)
            def health_check(self):
                self.client.get("/health")

            @task(2)
            def list_files(self):
                self.client.get("/files?file_type=jmx")

            @task(1)
            def list_tasks(self):
                self.client.get("/tasks")

            def on_start(self):
                # Create a test JMX file for upload
                self.jmx_content = '''<?xml version="1.0" encoding="UTF-8"?>
        <jmeterTestPlan version="1.2">
          <hashTree>
            <TestPlan testname="Load Test Plan"/>
          </hashTree>
        </jmeterTestPlan>'''
        EOF

    - name: Start application for load testing
      run: |
        export ENVIRONMENT=development
        export DATABASE_URL=sqlite:///./load_test.db
        python dev_server.py &
        sleep 10
        echo "Application started for load testing"

    - name: Run load test
      run: |
        locust --headless --users 10 --spawn-rate 2 --run-time 60s --host http://localhost:8000 --html load_test_report.html

    - name: Stop application
      run: |
        pkill -f "python dev_server.py" || true

    - name: Upload load test report
      uses: actions/upload-artifact@v4
      with:
        name: load-test-report
        path: load_test_report.html
